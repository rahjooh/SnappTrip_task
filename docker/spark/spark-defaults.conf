# Spark Configuration for SnappTrip Data Platform

# Adaptive Query Execution
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.adaptive.localShuffleReader.enabled=true

# Dynamic Allocation
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=2
spark.dynamicAllocation.maxExecutors=10
spark.dynamicAllocation.initialExecutors=2
spark.dynamicAllocation.executorIdleTimeout=60s

# Serialization
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false

# Memory Management
spark.memory.fraction=0.8
spark.memory.storageFraction=0.3

# Shuffle
spark.shuffle.compress=true
spark.shuffle.spill.compress=true
spark.io.compression.codec=snappy

# Parquet Optimization
spark.sql.parquet.enableVectorizedReader=true
spark.sql.parquet.compression.codec=snappy
spark.sql.parquet.filterPushdown=true
spark.sql.parquet.mergeSchema=false

# Arrow Optimization
spark.sql.execution.arrow.pyspark.enabled=true
spark.sql.execution.arrow.pyspark.fallback.enabled=true

# Iceberg Configuration
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog
spark.sql.catalog.spark_catalog.type=hive
spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.local.type=hadoop
spark.sql.catalog.local.warehouse=hdfs://namenode:9000/warehouse

# Kafka Configuration
spark.sql.streaming.kafka.useDeprecatedOffsetFetching=false

# Metrics
spark.metrics.conf.*.sink.prometheusServlet.class=org.apache.spark.metrics.sink.PrometheusServlet
spark.metrics.conf.*.sink.prometheusServlet.path=/metrics/prometheus
spark.ui.prometheus.enabled=true

# Event Log
spark.eventLog.enabled=true
spark.eventLog.dir=hdfs://namenode:9000/spark-logs
spark.history.fs.logDirectory=hdfs://namenode:9000/spark-logs

# Network
spark.network.timeout=300s
spark.executor.heartbeatInterval=30s

# UI
spark.ui.reverseProxy=true
spark.ui.reverseProxyUrl=/
# Kafka Connector JARs
spark.jars=/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar,/opt/spark/jars/kafka-clients-3.4.1.jar,/opt/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar,/opt/spark/jars/commons-pool2-2.11.1.jar
